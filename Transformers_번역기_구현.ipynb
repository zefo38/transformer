{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYSrkBX7IJcng9kiNUnhTJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# transformers\n",
        "-  Hugging Face에서 제공하는 NLP 모델 라이브러리로, BERT, GPT, RoBERTa, T5 등 다양한 사전 학습된 모델을 지원한다.\n",
        "- NLP와 트랜스포머 기반 딥러닝 연구를 간편하게 수행할 수 있도록 도와준다."
      ],
      "metadata": {
        "id": "h4tbSWhMhhnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrfR3gh-f38_",
        "outputId": "6898125c-00eb-4ebe-b32e-4d1960d56928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 3s (3,539 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font',family=\"NanumBarunGothic\")\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False"
      ],
      "metadata": {
        "id": "nHsA94JHgTO-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "gdown : google drive에 저장된 파일 다운로드할 수 있게 도와주는 패키지  \n",
        "(머신러닝 모델, 데이터셋)  \n",
        "einops : 딥러닝 작업에서 텐서의 재배열을 간단하게 처리하도록 도와주는 패키지  \n",
        "(transformers 모델처럼 복잡한 텐서 구조에 유용)  \n",
        "sentencepiece : 텍스트 토큰화, 언어에 구애받지 않고 BPE(Byte Pair Encoding)나 Unigram 모델 기반으로 서브워드 토큰화 제공  \n",
        "(Hugging Face와 같은 NLP 라이브러리와 함께 사용)  \n",
        "sacremoses : 텍스트 전처리, 텍스트를 토큰화하거나 역토큰화할 때 사용되며, 번역 모델의 전처리 단계에서 자주 사용\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2YQD7qgAgy3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip inatall einops\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Hpu9lggg2q",
        "outputId": "e2952673-b9d9-4c6b-94b1-7c0e864ceb48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "ERROR: unknown command \"inatall\" - maybe you meant \"install\"\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#\n",
        "train_model = False\n",
        "\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KoSXSmU2grUg",
        "outputId": "008f16ed-510a-4852-888d-55d2bd27e92f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 모델 구현  \n",
        "## 1. MHA ( Multi Head Attention )  \n",
        "Encoder, Decoder의 Self Attention을 수행하는 모듈  \n",
        "### 1.1 einops 사용하기  \n",
        "- 장점 : 텐서의 변환을 매우 쉽게 이해하고 변환 가능  \n",
        "- 단점 : 오류 잡기가 매우 힘들고 속도도 비교적 느리다"
      ],
      "metadata": {
        "id": "b9EIKGA2h4_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "# 차원 재배치\n",
        "x = torch.randn(2, 3, 4) # Tensor shape : (2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> c a b')\n",
        "\n",
        "y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Ej-RL6h3El",
        "outputId": "fb1123c0-c6fc-4f45-cf05-693a07b9a2f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 합치기\n",
        "x = torch.randn(2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> (a b) c')\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8va0BEuiYae",
        "outputId": "7f855559-9f0b-4651-ac57-159cf8407ee6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 나누기\n",
        "x = torch.randn(6, 4)\n",
        "y = rearrange(x, '(a b) c -> a b c', a=2, b=3)\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYUvKfBVilwj",
        "outputId": "94b79796-6b0e-40c0-8a30-2e630a2b32eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MHA 구현"
      ],
      "metadata": {
        "id": "MJdaVwrNi9M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import torch\n",
        "\n",
        "class MHA(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, n_heads=8):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원 (dimension of embedding vector)\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수 (number of heads in multi_head attention)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # Query, Key, Value 벡터를 위한 선형 레이어를 정의\n",
        "    self.fc_q = nn.Linear(d_model, d_model)\n",
        "    self.fc_k = nn.Linear(d_model, d_model)\n",
        "    self.fc_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 최종 출력을 위한 선형 레이어\n",
        "    self.fc_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 어텐션 스코어를 스케일링 하기 위한 값. (d_model / n_heads의 제곱근)\n",
        "    self.scale = torch.sqrt(torch.tensor(d_model / n_heads)) # 512 / 8의 root\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "      Q, K, V : 단어가 임베딩 레이어를 통과한 결과. : 임베딩 벡터\n",
        "      shape : (N, t, D) : (batch_size, max_len, dimension)\n",
        "      mask : 어텐션 스코어에 적용할 마스트 (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    # 임베딩 벡터인 Q, K, V에 대한 선형 변환 적용\n",
        "    Q = self.fc_q(Q) # (N, t, D) -> (N, t, D)\n",
        "    K = self.fc_k(K) # (N, t, D) -> (N, t, D)\n",
        "    V = self.fc_v(V) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    # 멀티 헤드를 위해 임베딩 차원 D를 헤드 개수 n_heads로 분할\n",
        "    # 예를 들어 (32, 128, 512)로 들어왔을 때 8개의 head로 나누면 (32, 8, 128, 64)\n",
        "    Q = rearrange(Q, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    K = rearrange(K, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    V = rearrange(V, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "\n",
        "    # 어텐션 스코어 구하기 (softmax 통과 전) QK^T / 루트d_k\n",
        "    # Q와 K^T의 내적을 통해 어텐션 스코어를 계산하고 스케일링 적용\n",
        "\n",
        "    # (N, h, t, dk) @ (N, h, dk, t) -> (N, h, t(query의 길이), t(Key의 길이))\n",
        "    attention_score = Q @ K.transpose(-2, -1) / self.scale\n",
        "\n",
        "    # 패딩의 위치에다가 굉장히 작은 값을 강제로 부여 -> 소프트맥스 적용 시에 0이 될 수 있도록\n",
        "    if mask is not None: # 패딩 위치를 의미하는 인덱스들이 존재한다면\n",
        "      attention_score[mask] = -1e10 # mask에 해당하는 위치에 굉장히 작은 값을 넣어준다\n",
        "\n",
        "    # 에너지 값을 구하기 위해서는 키의 방향으로 softmax를 적용한다\n",
        "    energy = torch.softmax(attention_score, dim=-1)\n",
        "\n",
        "    # 에너지와 V를 곱해서 최종 어텐션 값을 구한다\n",
        "    attention = energy @ V # (N, h, t, t) @ (N, h, t, dk) -> (N, h, t, dk)\n",
        "\n",
        "    # 헤드 차원을 연결해서 원래의 차원으로 되돌리기\n",
        "    x = rearrange(attention, 'N h t dk -> N t (h dk)') # (N, h, t, dk) -> (N, t, D)\n",
        "\n",
        "    # 최종 출력값에 대해 선형 변환을 적용한다 각각의 헤드의 생각을 섞어준다.\n",
        "    x = self.fx_o(x) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    return x, energy"
      ],
      "metadata": {
        "id": "-wyWksH7i78C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. FNN (Feed Forward Network)  \n",
        "- 인코더와 디코더의 MHA의 결과를 하나로 합쳐주는 역할"
      ],
      "metadata": {
        "id": "adH2fDP3QwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, d_ff=2048, drop_p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear == nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(drop_p),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, mha_output):\n",
        "    out = self.linear(mha_output)\n",
        "    return out"
      ],
      "metadata": {
        "id": "WpVaykQhQv2B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder 구현"
      ],
      "metadata": {
        "id": "O3S3TsPCRVwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MHA - FFN 연결 과정 구현\n",
        "# 추가적으로 skip connection, LN까지 구현\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원\n",
        "      d_ff : 피드 포워드 신경망의 은닉층의 차원\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
        "      drop_p : 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi Head Attention 레이어 정의 (self-attention)\n",
        "    self.self_atten = MHA(d_model, n_heads)\n",
        "\n",
        "    # MHA에 대한 Layer Normalization 정의\n",
        "    self.self_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 피드 포워드 네트워크 정의\n",
        "    self.FF = FeedForward(d_model, d_ff, drop_p)\n",
        "\n",
        "    # 피드 포워드 네트워크 출력에 대한 Layer Normalization\n",
        "    self.FF_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 드롭아웃 레이어 정의\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x, enc_mask):\n",
        "    \"\"\"\n",
        "      x : 입력 텐서, Shape은 (batch_size, seq_len, d_model) -> Projection Layer\n",
        "      enc_mask : 입력 마스크, Shape은 (batch_size, 1, seq_len)\n",
        "\n",
        "      return : 인코더 레이어의 출력과 어텐션 가중치(에너지)\n",
        "    \"\"\"\n",
        "\n",
        "    # Multi Head Attention 블록 구현\n",
        "    # Linear Layer를 지나기 전에는 Q=K=V다. x로 받아오고 있음\n",
        "\n",
        "    residual, atten_enc = self.self_atten(Q=x, K=x, V=x, mask=enc_mask)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection(Add) & Layer Norm\n",
        "    encoder_self_attention_output = self.self_atten_LN(x + residual)\n",
        "\n",
        "    # FFN 블록\n",
        "    residual = self.FF(encoder_self_attention_output)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection & Layer Norm\n",
        "    encoder_ffn_output = self.FF_LN(encoder_self_attention_output + residual)\n",
        "\n",
        "    return encoder_ffn_output, atten_enc"
      ],
      "metadata": {
        "id": "x-yd0aPqi3PZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F7s6CBMAYybf"
      }
    }
  ]
}