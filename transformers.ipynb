{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgUdVLChkqAyHIgYrcBoW/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# transformers\n",
        "-  Hugging Face에서 제공하는 NLP 모델 라이브러리로, BERT, GPT, RoBERTa, T5 등 다양한 사전 학습된 모델을 지원한다.\n",
        "- NLP와 트랜스포머 기반 딥러닝 연구를 간편하게 수행할 수 있도록 도와준다."
      ],
      "metadata": {
        "id": "h4tbSWhMhhnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrfR3gh-f38_",
        "outputId": "61b6fb62-1a00-4ce0-9dc6-a9a434abd6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (14.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font',family=\"NanumBarunGothic\")\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False"
      ],
      "metadata": {
        "id": "nHsA94JHgTO-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "gdown : google drive에 저장된 파일 다운로드할 수 있게 도와주는 패키지  \n",
        "(머신러닝 모델, 데이터셋)  \n",
        "einops : 딥러닝 작업에서 텐서의 재배열을 간단하게 처리하도록 도와주는 패키지  \n",
        "(transformers 모델처럼 복잡한 텐서 구조에 유용)  \n",
        "sentencepiece : 텍스트 토큰화, 언어에 구애받지 않고 BPE(Byte Pair Encoding)나 Unigram 모델 기반으로 서브워드 토큰화 제공  \n",
        "(Hugging Face와 같은 NLP 라이브러리와 함께 사용)  \n",
        "sacremoses : 텍스트 전처리, 텍스트를 토큰화하거나 역토큰화할 때 사용되며, 번역 모델의 전처리 단계에서 자주 사용\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2YQD7qgAgy3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip inatall einops\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Hpu9lggg2q",
        "outputId": "5403a285-6944-4c19-f98c-0a4ba434c04b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "ERROR: unknown command \"inatall\" - maybe you meant \"install\"\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#\n",
        "train_model = False\n",
        "\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KoSXSmU2grUg",
        "outputId": "cebb1192-d0f1-4b82-c3ba-f93ad1c08bf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 모델 구현  \n",
        "## 1. MHA ( Multi Head Attention )  \n",
        "Encoder, Decoder의 Self Attention을 수행하는 모듈  \n",
        "### 1.1 einops 사용하기  \n",
        "- 장점 : 텐서의 변환을 매우 쉽게 이해하고 변환 가능  \n",
        "- 단점 : 오류 잡기가 매우 힘들고 속도도 비교적 느리다"
      ],
      "metadata": {
        "id": "b9EIKGA2h4_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "# 차원 재배치\n",
        "x = torch.randn(2, 3, 4) # Tensor shape : (2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> c a b')\n",
        "\n",
        "y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Ej-RL6h3El",
        "outputId": "1bc58ded-6c70-470d-9516-26991c7123a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 합치기\n",
        "x = torch.randn(2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> (a b) c')\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8va0BEuiYae",
        "outputId": "11c89786-1ca6-4490-985e-b32131dacc3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 나누기\n",
        "x = torch.randn(6, 4)\n",
        "y = rearrange(x, '(a b) c -> a b c', a=2, b=3)\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYUvKfBVilwj",
        "outputId": "ddc4ab26-dfd2-4abb-896c-1738a4592620"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MHA 구현"
      ],
      "metadata": {
        "id": "MJdaVwrNi9M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import torch\n",
        "\n",
        "class MHA(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, n_heads=8):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원 (dimension of embedding vector)\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수 (number of heads in multi_head attention)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # Query, Key, Value 벡터를 위한 선형 레이어를 정의\n",
        "    self.fc_q = nn.Linear(d_model, d_model)\n",
        "    self.fc_k = nn.Linear(d_model, d_model)\n",
        "    self.fc_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 최종 출력을 위한 선형 레이어\n",
        "    self.fc_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 어텐션 스코어를 스케일링 하기 위한 값. (d_model / n_heads의 제곱근)\n",
        "    self.scale = torch.sqrt(torch.tensor(d_model / n_heads)) # 512 / 8의 root\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "      Q, K, V : 단어가 임베딩 레이어를 통과한 결과. : 임베딩 벡터\n",
        "      shape : (N, t, D) : (batch_size, max_len, dimension)\n",
        "      mask : 어텐션 스코어에 적용할 마스트 (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    # 임베딩 벡터인 Q, K, V에 대한 선형 변환 적용\n",
        "    Q = self.fc_q(Q) # (N, t, D) -> (N, t, D)\n",
        "    K = self.fc_k(K) # (N, t, D) -> (N, t, D)\n",
        "    V = self.fc_v(V) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    # 멀티 헤드를 위해 임베딩 차원 D를 헤드 개수 n_heads로 분할\n",
        "    # 예를 들어 (32, 128, 512)로 들어왔을 때 8개의 head로 나누면 (32, 8, 128, 64)\n",
        "    Q = rearrange(Q, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    K = rearrange(K, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    V = rearrange(V, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "\n",
        "    # 어텐션 스코어 구하기 (softmax 통과 전) QK^T / 루트d_k\n",
        "    # Q와 K^T의 내적을 통해 어텐션 스코어를 계산하고 스케일링 적용\n",
        "\n",
        "    # (N, h, t, dk) @ (N, h, dk, t) -> (N, h, t(query의 길이), t(Key의 길이))\n",
        "    attention_score = Q @ K.transpose(-2, -1) / self.scale\n",
        "\n",
        "    # 패딩의 위치에다가 굉장히 작은 값을 강제로 부여 -> 소프트맥스 적용 시에 0이 될 수 있도록\n",
        "    if mask is not None: # 패딩 위치를 의미하는 인덱스들이 존재한다면\n",
        "      attention_score[mask] = -1e10 # mask에 해당하는 위치에 굉장히 작은 값을 넣어준다\n",
        "\n",
        "    # 에너지 값을 구하기 위해서는 키의 방향으로 softmax를 적용한다\n",
        "    energy = torch.softmax(attention_score, dim=-1)\n",
        "\n",
        "    # 에너지와 V를 곱해서 최종 어텐션 값을 구한다\n",
        "    attention = energy @ V # (N, h, t, t) @ (N, h, t, dk) -> (N, h, t, dk)\n",
        "\n",
        "    # 헤드 차원을 연결해서 원래의 차원으로 되돌리기\n",
        "    x = rearrange(attention, 'N h t dk -> N t (h dk)') # (N, h, t, dk) -> (N, t, D)\n",
        "\n",
        "    # 최종 출력값에 대해 선형 변환을 적용한다 각각의 헤드의 생각을 섞어준다.\n",
        "    x = self.fx_o(x) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    return x, energy"
      ],
      "metadata": {
        "id": "-wyWksH7i78C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. FNN (Feed Forward Network)  \n",
        "- 인코더와 디코더의 MHA의 결과를 하나로 합쳐주는 역할"
      ],
      "metadata": {
        "id": "adH2fDP3QwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, d_ff=2048, drop_p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear == nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(drop_p),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, mha_output):\n",
        "    out = self.linear(mha_output)\n",
        "    return out"
      ],
      "metadata": {
        "id": "WpVaykQhQv2B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder 구현"
      ],
      "metadata": {
        "id": "O3S3TsPCRVwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MHA - FFN 연결 과정 구현\n",
        "# 추가적으로 skip connection, LN까지 구현\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원\n",
        "      d_ff : 피드 포워드 신경망의 은닉층의 차원\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
        "      drop_p : 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi Head Attention 레이어 정의 (self-attention)\n",
        "    self.self_atten = MHA(d_model, n_heads)\n",
        "\n",
        "    # MHA에 대한 Layer Normalization 정의\n",
        "    self.self_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 피드 포워드 네트워크 정의\n",
        "    self.FF = FeedForward(d_model, d_ff, drop_p)\n",
        "\n",
        "    # 피드 포워드 네트워크의 출력에 대한 Layer Normailzation\n",
        "    self.FF_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 드롭아웃 레이어 정의\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "  def forward(self, x, enc_mask):\n",
        "\n",
        "    \"\"\"\n",
        "      x : 입력 텐서, Shape은 (batch_size, seq_len, d_model) -> Projection Layer\n",
        "      enc_mask : 입력 마스크, Shape은 (batch_size, 1, seq_len)\n",
        "\n",
        "      return : 인코더 레이어의 출력과 어텐션 가중치(에너지)\n",
        "    \"\"\"\n",
        "\n",
        "    # Multi Head Attention 블록 구현\n",
        "    # 리니어 레이어를 지나기 전에는 Q=K=V다. 이를 x로 받아오고 있음\n",
        "    residual, atten_enc = self.self_atten(Q=x, K=x, V=x, mask=enc_mask)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection(Add) & Layer Norm\n",
        "    encoder_self_attention_output = self.self_atten_LN(x + residual)\n",
        "\n",
        "    # FFN 블록\n",
        "    residual = self.FF(encoder_self_attention_output)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection & Layer Norm\n",
        "    encoder_ffn_output = self.FF_LN(encoder_self_attention_output + residual)\n",
        "\n",
        "    return encoder_ffn_output, atten_enc"
      ],
      "metadata": {
        "id": "x-yd0aPqi3PZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n",
        "    \"\"\"\n",
        "      input_embedding : 입력 임베딩 레이어 (nn.Embedding)\n",
        "      max_len : 입력 시퀀스의 최대 길이 (int)\n",
        "      n_layers : 인코더 레이어의 개수\n",
        "      d_model : 임베딩 벡터의 차원\n",
        "      d_ff : 피드 포워드 신경망의 은닉층의 차원\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
        "      drop_p : 드롭아웃 비율\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # d_model의 제곱근 값으로 scale을 정의하여 임베딩 벡터의 크기를 조정\n",
        "    self.scale = torch.sqrt(torch.tensor(d_model))\n",
        "\n",
        "    # 입력 임베딩 레이어\n",
        "    self.input_embeddin = input_embedding\n",
        "\n",
        "    # 위치 임베딩 레이어 : 입력 시퀀스의 위치 정보를 학습하기 위한 레이어.\n",
        "    # 최근에는 위치 정보도 학습의 대상으로 보기 위함\n",
        "    self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    # 여러 개의 인코더 레이어를 쌓기 위해 모듈 리스트를 활용\n",
        "    self.layers = nn.ModuleList(\n",
        "        [ EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)]\n",
        "    )\n",
        "    self.device = DEVICE\n",
        "\n",
        "  def forward(self, src, mask, atten_map_save=False):\n",
        "    \"\"\"\n",
        "      src : 입력 시퀀스 (batch_size, seq_len)\n",
        "      mask : 패딩을 마스킹하기 위한 마스크 텐서 (batch_size, 1, seq_len)\n",
        "      atten_map_save : 어켄션 맵을 저장할지 여부 (기본값 : False)\n",
        "    \"\"\"\n",
        "\n",
        "    # 위치 인덱스 텐서 생성 : 각 배치에서 시퀀스의 길이만큼 위치 인덱스 반복\n",
        "    pos = torch.arrange(src.shape[1]).repeat(src.shape[0], 1).to(self.device)\n",
        "\n",
        "    # 입력 임베딩과 위치 임베딩을 합산해서 입력 텐서 x 생성\n",
        "    x_embedding = self.scale * self.input_embedding(src) + self.pos_embedding(pos)\n",
        "\n",
        "    # 드롭아웃\n",
        "    x_embedding = self.dropout(x_embedding)\n",
        "\n",
        "    # 제일 처음 입력\n",
        "    encoder_output = x_embedding\n",
        "\n",
        "    # 인코더의 에너지 값들을 저장할 비어있는 텐서\n",
        "    atten_encs = torch.tensor([]).to(self.device)\n",
        "\n",
        "    # 각 인코더 레이어를 순차적으로 통과\n",
        "    for layer in self.layers:\n",
        "      # 인코더 레이어를 통과시키면 encoder_output과 에너지 값을 얻는다.\n",
        "      encoder_output, atten_enc = layer(encoder_output, mask)\n",
        "\n",
        "      # atten_map_save가 True면 에너지 저장\n",
        "      if atten_map_save:\n",
        "\n",
        "        # 현재 어텐션 맵을 기존의 atten_encs에 추가 (첫 번째 레이어의 어텐션 맵만 저장 )\n",
        "        atten_encs = torch.cat([atten_encs, atten_enc[0].unsqueeze(0)], dim=0)\n",
        "\n",
        "      # 최종 출력과 에너지 맵을 반환\n",
        "      return encoder_output, atten_encs"
      ],
      "metadata": {
        "id": "7QCld9YPKg6F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Decoder 구현  \n",
        "**Masking 실험**"
      ],
      "metadata": {
        "id": "hglUMTRWQfJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 예제 설정\n",
        "batch_size = 3\n",
        "seq_len = 10\n",
        "padding = 3\n",
        "n_heads = 8\n",
        "\n",
        "# attention_score 텐서 생성 (무작위 값으로 초기화)\n",
        "# Shape : (batch_size, n_heads, seq_len, seq_len)\n",
        "attention_score = torch.randn(batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "# enc_mask 생성 : 패딩이 있는 위치를 마스킹\n",
        "# 각 시퀀스의 마지막 3개 위치에 패딩이 있다고 가정\n",
        "enc_mask = torch.tensor([\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 첫 번째 시퀀스\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 두 번째 시퀀스\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 세 번째 시퀀스\n",
        "], dtype = torch.bool).unsqueeze(1).unsqueeze(2) # Shape : (batch_size, 1, 1, seq_len)\n",
        "\n",
        "# enc_mask의 shape을 (batch_size, n_heads, seq_len, seq_len)로 확장\n",
        "enc_mask = enc_mask.expand(batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 전\", \"=\"*25)\n",
        "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\", attention_score[0,0])\n",
        "\n",
        "# attention_score에 enc_mask 적용\n",
        "if enc_mask is not None:\n",
        "  attention_score[enc_mask] = 0 # 마스크된 위치에 매우 작은 값을 넣어 softmax 결과에 영향 미치지 않게 함\n",
        "\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 전\", \"=\"*25)\n",
        "print(\"attention_score shape:\", attention_score.shape)\n",
        "print(enc_mask.shape)\n",
        "\n",
        "# 특정 헤드에 대한 attention_score 확인 (첫 번재 헤드)\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 후\", \"=\"*25)\n",
        "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\", attention_score[0, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E98shYQqLu0v",
        "outputId": "7100c5e3-7afc-44e4-810e-0e99d381ce34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================= attention_score에 마스킹 적용 전 =========================\n",
            "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
            " tensor([[ 1.4086e+00,  7.0849e-01,  2.6769e-01, -2.7591e-02,  5.2874e-01,\n",
            "          1.0179e+00,  4.3681e-01,  7.1985e-01,  4.3365e-01,  1.7712e+00],\n",
            "        [-8.4898e-01,  3.5340e-01, -5.9011e-01, -1.9423e+00, -6.3491e-01,\n",
            "         -1.3555e+00, -1.5007e+00,  3.3165e-01,  1.2865e+00, -7.0250e-02],\n",
            "        [ 7.9057e-01,  1.7039e+00,  6.4460e-02, -7.8073e-01, -9.0764e-01,\n",
            "         -7.6891e-01,  2.7758e-02, -5.7297e-01,  1.4036e+00,  4.9849e-01],\n",
            "        [ 4.2384e-01, -5.5427e-01,  1.2881e-01, -1.1226e+00,  4.3744e-01,\n",
            "         -2.5542e-05,  1.1006e+00, -2.2449e+00,  8.7916e-01, -1.7438e+00],\n",
            "        [-7.3371e-02,  1.1193e+00, -2.0132e+00,  2.4533e-01,  1.1731e+00,\n",
            "         -6.6114e-02,  1.5799e+00,  1.4119e+00, -8.6355e-01,  2.0156e-01],\n",
            "        [-4.8999e-01,  1.9785e+00, -2.1782e+00,  2.2810e+00, -7.1590e-01,\n",
            "          7.0295e-01,  1.9300e-01,  1.5347e-01, -1.1260e+00, -1.3306e+00],\n",
            "        [-6.7815e-01,  2.7048e-01,  8.7626e-01,  1.3881e+00,  4.8415e-01,\n",
            "          8.2837e-02, -6.2030e-02,  5.0353e-01,  4.5664e-01, -8.1796e-01],\n",
            "        [ 7.2337e-01, -1.4360e-01,  1.5001e+00,  5.0419e-01, -3.2816e-02,\n",
            "          7.9144e-01, -2.0754e+00, -8.5200e-01, -2.7078e-02,  2.2075e-01],\n",
            "        [ 2.1072e+00,  6.3650e-01,  5.6148e-01,  8.4752e-01, -4.9546e-01,\n",
            "          1.1629e+00, -2.2581e+00,  1.3745e+00,  1.5269e-01, -1.0969e+00],\n",
            "        [ 2.2440e-01,  1.2546e+00, -1.2314e+00,  1.7696e+00,  1.2389e-01,\n",
            "          1.5610e+00,  6.3935e-01,  6.7817e-01, -7.8314e-02, -5.7841e-01]])\n",
            "========================= attention_score에 마스킹 적용 전 =========================\n",
            "attention_score shape: torch.Size([3, 8, 10, 10])\n",
            "torch.Size([3, 8, 10, 10])\n",
            "========================= attention_score에 마스킹 적용 후 =========================\n",
            "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
            " tensor([[ 1.4086e+00,  7.0849e-01,  2.6769e-01, -2.7591e-02,  5.2874e-01,\n",
            "          1.0179e+00,  4.3681e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.4898e-01,  3.5340e-01, -5.9011e-01, -1.9423e+00, -6.3491e-01,\n",
            "         -1.3555e+00, -1.5007e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 7.9057e-01,  1.7039e+00,  6.4460e-02, -7.8073e-01, -9.0764e-01,\n",
            "         -7.6891e-01,  2.7758e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.2384e-01, -5.5427e-01,  1.2881e-01, -1.1226e+00,  4.3744e-01,\n",
            "         -2.5542e-05,  1.1006e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-7.3371e-02,  1.1193e+00, -2.0132e+00,  2.4533e-01,  1.1731e+00,\n",
            "         -6.6114e-02,  1.5799e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-4.8999e-01,  1.9785e+00, -2.1782e+00,  2.2810e+00, -7.1590e-01,\n",
            "          7.0295e-01,  1.9300e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-6.7815e-01,  2.7048e-01,  8.7626e-01,  1.3881e+00,  4.8415e-01,\n",
            "          8.2837e-02, -6.2030e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 7.2337e-01, -1.4360e-01,  1.5001e+00,  5.0419e-01, -3.2816e-02,\n",
            "          7.9144e-01, -2.0754e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.1072e+00,  6.3650e-01,  5.6148e-01,  8.4752e-01, -4.9546e-01,\n",
            "          1.1629e+00, -2.2581e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.2440e-01,  1.2546e+00, -1.2314e+00,  1.7696e+00,  1.2389e-01,\n",
            "          1.5610e+00,  6.3935e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
        "    super().__init__()\n",
        "\n",
        "    # Decoder의 Self-Attention Layer -> Masked Multi Head Attention 정의\n",
        "    self.self_atten = MHA(d_model, n_heads) # forward할 때 Decoder Mask가 따로 부여\n",
        "    self.self_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Encoder - Decoder Attention Layer\n",
        "    self.enc_dec_atten = MHA(d_model, n_heads)\n",
        "    self.enc_dec_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Feed Forward\n",
        "    self.FF = FeedForward(d_model, d_ff, drop_p)\n",
        "    self.FF_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Dropout\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "  def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
        "    \"\"\"\n",
        "      x : 디코더의 입력 텐서 (batch_size, seq_len, d_model)\n",
        "      enc_out : 인코더의 출력 텐서 (batch_size, seq_len, d_model)\n",
        "      dec_mask : 디코더의 Self-Attention에 사용되는 마스크 (batch_size, 1, seq_len)\n",
        "      enc_dec_mask : 인코더-디코더 Attention에 사용되는 마스크 (batch_size, 1, seq_len)\n",
        "\n",
        "      return : 디코더 레이어의 출력 텐서, 디코더 Self-Attention 맵, 인코더-디코더 Attention 맵\n",
        "    \"\"\"\n",
        "    # 1. 디코더의 Self Attention\n",
        "    residual, atten_dec = self.self_atten(Q=x, K=x, V=x, mask=dec_mask)\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_masked_self_attention_output = self.self_atten_LN(x + residual)\n",
        "\n",
        "    # 2. Encoder - Decoder Attention\n",
        "    # Q는 Masked Multi Head Attention, K, V는 인코더의 출력\n",
        "    residual, atten_dec_enc = self.enc_dec_atten(\n",
        "        Q=decoder_masked_self_attention_output,\n",
        "        K=enc_out,\n",
        "        V=enc_out,\n",
        "        mask=enc_dec_mask\n",
        "    )\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_self_attention_output = self.enc_dec_atten_LN(x + residual)\n",
        "\n",
        "    # 3. Feed Forward\n",
        "    residual = self.FF(decoder_self_attention_output)\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_output = self.FF_LN(decoder_self_attention_output + residual)\n",
        "\n",
        "    # decoder_output : 디코더 출력값. 디코더가 N회 반복된 후 소프트맥스를 만나 단어를 출력하기 위한 값\n",
        "    # atten_dec : Masked Self Attention의 에너지 맵\n",
        "    # atten_dec_enc : Encoder에서 출력한 단어와 Decoder에서 출력할 단어의 에너지 맵\n",
        "    return decoder_output, atten_dec, atten_dec_enc"
      ],
      "metadata": {
        "id": "A07pqWVlSwcu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWigq67RXMYw"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}