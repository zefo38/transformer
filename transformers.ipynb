{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXpKEBJi1z0NdJppY/FPw+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# transformers\n",
        "-  Hugging Face에서 제공하는 NLP 모델 라이브러리로, BERT, GPT, RoBERTa, T5 등 다양한 사전 학습된 모델을 지원한다.\n",
        "- NLP와 트랜스포머 기반 딥러닝 연구를 간편하게 수행할 수 있도록 도와준다."
      ],
      "metadata": {
        "id": "h4tbSWhMhhnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrfR3gh-f38_",
        "outputId": "942e16a8-47bf-49e8-fe9a-126455f42a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 2s (6,179 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font',family=\"NanumBarunGothic\")\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False"
      ],
      "metadata": {
        "id": "nHsA94JHgTO-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "gdown : google drive에 저장된 파일 다운로드할 수 있게 도와주는 패키지  \n",
        "(머신러닝 모델, 데이터셋)  \n",
        "einops : 딥러닝 작업에서 텐서의 재배열을 간단하게 처리하도록 도와주는 패키지  \n",
        "(transformers 모델처럼 복잡한 텐서 구조에 유용)  \n",
        "sentencepiece : 텍스트 토큰화, 언어에 구애받지 않고 BPE(Byte Pair Encoding)나 Unigram 모델 기반으로 서브워드 토큰화 제공  \n",
        "(Hugging Face와 같은 NLP 라이브러리와 함께 사용)  \n",
        "sacremoses : 텍스트 전처리, 텍스트를 토큰화하거나 역토큰화할 때 사용되며, 번역 모델의 전처리 단계에서 자주 사용\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2YQD7qgAgy3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip inatall einops\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Hpu9lggg2q",
        "outputId": "3b2fc2ca-db12-416f-e3b3-cfcfe538b766"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "ERROR: unknown command \"inatall\" - maybe you meant \"install\"\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.67.1)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#\n",
        "train_model = False\n",
        "\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KoSXSmU2grUg",
        "outputId": "ff4d24d5-bc83-4559-dd6a-cc210a4017d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 모델 구현  \n",
        "## 1. MHA ( Multi Head Attention )  \n",
        "Encoder, Decoder의 Self Attention을 수행하는 모듈  \n",
        "### 1.1 einops 사용하기  \n",
        "- 장점 : 텐서의 변환을 매우 쉽게 이해하고 변환 가능  \n",
        "- 단점 : 오류 잡기가 매우 힘들고 속도도 비교적 느리다"
      ],
      "metadata": {
        "id": "b9EIKGA2h4_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange\n",
        "\n",
        "# 차원 재배치\n",
        "x = torch.randn(2, 3, 4) # Tensor shape : (2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> c a b')\n",
        "\n",
        "y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Ej-RL6h3El",
        "outputId": "2f94d682-aa2a-46e5-a007-2e4eeb85e2eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 합치기\n",
        "x = torch.randn(2, 3, 4)\n",
        "y = rearrange(x, 'a b c -> (a b) c')\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8va0BEuiYae",
        "outputId": "299e924d-79ca-42b4-f8e8-3ec175267f6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 차원 나누기\n",
        "x = torch.randn(6, 4)\n",
        "y = rearrange(x, '(a b) c -> a b c', a=2, b=3)\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYUvKfBVilwj",
        "outputId": "a3b9a6cf-9f9e-4d46-941e-2e55255ceee2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MHA 구현"
      ],
      "metadata": {
        "id": "MJdaVwrNi9M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from einops import rearrange\n",
        "import torch\n",
        "\n",
        "class MHA(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, n_heads=8):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원 (dimension of embedding vector)\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수 (number of heads in multi_head attention)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # Query, Key, Value 벡터를 위한 선형 레이어를 정의\n",
        "    self.fc_q = nn.Linear(d_model, d_model)\n",
        "    self.fc_k = nn.Linear(d_model, d_model)\n",
        "    self.fc_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 최종 출력을 위한 선형 레이어\n",
        "    self.fc_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    # 어텐션 스코어를 스케일링 하기 위한 값. (d_model / n_heads의 제곱근)\n",
        "    self.scale = torch.sqrt(torch.tensor(d_model / n_heads)) # 512 / 8의 root\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "      Q, K, V : 단어가 임베딩 레이어를 통과한 결과. : 임베딩 벡터\n",
        "      shape : (N, t, D) : (batch_size, max_len, dimension)\n",
        "      mask : 어텐션 스코어에 적용할 마스트 (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    # 임베딩 벡터인 Q, K, V에 대한 선형 변환 적용\n",
        "    Q = self.fc_q(Q) # (N, t, D) -> (N, t, D)\n",
        "    K = self.fc_k(K) # (N, t, D) -> (N, t, D)\n",
        "    V = self.fc_v(V) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    # 멀티 헤드를 위해 임베딩 차원 D를 헤드 개수 n_heads로 분할\n",
        "    # 예를 들어 (32, 128, 512)로 들어왔을 때 8개의 head로 나누면 (32, 8, 128, 64)\n",
        "    Q = rearrange(Q, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    K = rearrange(K, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "    V = rearrange(V, 'N t (h dk) -> N h t dk', h=self.n_heads) # (N, t, D) -> (N, h, t, D//h)\n",
        "\n",
        "    # 어텐션 스코어 구하기 (softmax 통과 전) QK^T / 루트d_k\n",
        "    # Q와 K^T의 내적을 통해 어텐션 스코어를 계산하고 스케일링 적용\n",
        "\n",
        "    # (N, h, t, dk) @ (N, h, dk, t) -> (N, h, t(query의 길이), t(Key의 길이))\n",
        "    attention_score = Q @ K.transpose(-2, -1) / self.scale\n",
        "\n",
        "    # 패딩의 위치에다가 굉장히 작은 값을 강제로 부여 -> 소프트맥스 적용 시에 0이 될 수 있도록\n",
        "    if mask is not None: # 패딩 위치를 의미하는 인덱스들이 존재한다면\n",
        "      attention_score[mask] = -1e10 # mask에 해당하는 위치에 굉장히 작은 값을 넣어준다\n",
        "\n",
        "    # 에너지 값을 구하기 위해서는 키의 방향으로 softmax를 적용한다\n",
        "    energy = torch.softmax(attention_score, dim=-1)\n",
        "\n",
        "    # 에너지와 V를 곱해서 최종 어텐션 값을 구한다\n",
        "    attention = energy @ V # (N, h, t, t) @ (N, h, t, dk) -> (N, h, t, dk)\n",
        "\n",
        "    # 헤드 차원을 연결해서 원래의 차원으로 되돌리기\n",
        "    x = rearrange(attention, 'N h t dk -> N t (h dk)') # (N, h, t, dk) -> (N, t, D)\n",
        "\n",
        "    # 최종 출력값에 대해 선형 변환을 적용한다 각각의 헤드의 생각을 섞어준다.\n",
        "    x = self.fx_o(x) # (N, t, D) -> (N, t, D)\n",
        "\n",
        "    return x, energy"
      ],
      "metadata": {
        "id": "-wyWksH7i78C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. FNN (Feed Forward Network)  \n",
        "- 인코더와 디코더의 MHA의 결과를 하나로 합쳐주는 역할"
      ],
      "metadata": {
        "id": "adH2fDP3QwiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=512, d_ff=2048, drop_p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear == nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(drop_p),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, mha_output):\n",
        "    out = self.linear(mha_output)\n",
        "    return out"
      ],
      "metadata": {
        "id": "WpVaykQhQv2B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder 구현"
      ],
      "metadata": {
        "id": "O3S3TsPCRVwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MHA - FFN 연결 과정 구현\n",
        "# 추가적으로 skip connection, LN까지 구현\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
        "    \"\"\"\n",
        "      d_model : 임베딩 벡터의 차원\n",
        "      d_ff : 피드 포워드 신경망의 은닉층의 차원\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
        "      drop_p : 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi Head Attention 레이어 정의 (self-attention)\n",
        "    self.self_atten = MHA(d_model, n_heads)\n",
        "\n",
        "    # MHA에 대한 Layer Normalization 정의\n",
        "    self.self_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 피드 포워드 네트워크 정의\n",
        "    self.FF = FeedForward(d_model, d_ff, drop_p)\n",
        "\n",
        "    # 피드 포워드 네트워크의 출력에 대한 Layer Normailzation\n",
        "    self.FF_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # 드롭아웃 레이어 정의\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "  def forward(self, x, enc_mask):\n",
        "\n",
        "    \"\"\"\n",
        "      x : 입력 텐서, Shape은 (batch_size, seq_len, d_model) -> Projection Layer\n",
        "      enc_mask : 입력 마스크, Shape은 (batch_size, 1, seq_len)\n",
        "\n",
        "      return : 인코더 레이어의 출력과 어텐션 가중치(에너지)\n",
        "    \"\"\"\n",
        "\n",
        "    # Multi Head Attention 블록 구현\n",
        "    # 리니어 레이어를 지나기 전에는 Q=K=V다. 이를 x로 받아오고 있음\n",
        "    residual, atten_enc = self.self_atten(Q=x, K=x, V=x, mask=enc_mask)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection(Add) & Layer Norm\n",
        "    encoder_self_attention_output = self.self_atten_LN(x + residual)\n",
        "\n",
        "    # FFN 블록\n",
        "    residual = self.FF(encoder_self_attention_output)\n",
        "    residual = self.dropout(residual)\n",
        "\n",
        "    # Skip Connection & Layer Norm\n",
        "    encoder_ffn_output = self.FF_LN(encoder_self_attention_output + residual)\n",
        "\n",
        "    return encoder_ffn_output, atten_enc"
      ],
      "metadata": {
        "id": "x-yd0aPqi3PZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n",
        "    \"\"\"\n",
        "      input_embedding : 입력 임베딩 레이어 (nn.Embedding)\n",
        "      max_len : 입력 시퀀스의 최대 길이 (int)\n",
        "      n_layers : 인코더 레이어의 개수\n",
        "      d_model : 임베딩 벡터의 차원\n",
        "      d_ff : 피드 포워드 신경망의 은닉층의 차원\n",
        "      n_heads : 멀티 헤드 어텐션의 헤드 개수\n",
        "      drop_p : 드롭아웃 비율\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # d_model의 제곱근 값으로 scale을 정의하여 임베딩 벡터의 크기를 조정\n",
        "    self.scale = torch.sqrt(torch.tensor(d_model))\n",
        "\n",
        "    # 입력 임베딩 레이어\n",
        "    self.input_embeddin = input_embedding\n",
        "\n",
        "    # 위치 임베딩 레이어 : 입력 시퀀스의 위치 정보를 학습하기 위한 레이어.\n",
        "    # 최근에는 위치 정보도 학습의 대상으로 보기 위함\n",
        "    self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "    # 여러 개의 인코더 레이어를 쌓기 위해 모듈 리스트를 활용\n",
        "    self.layers = nn.ModuleList(\n",
        "        [ EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)]\n",
        "    )\n",
        "    self.device = DEVICE\n",
        "\n",
        "  def forward(self, src, mask, atten_map_save=False):\n",
        "    \"\"\"\n",
        "      src : 입력 시퀀스 (batch_size, seq_len)\n",
        "      mask : 패딩을 마스킹하기 위한 마스크 텐서 (batch_size, 1, seq_len)\n",
        "      atten_map_save : 어켄션 맵을 저장할지 여부 (기본값 : False)\n",
        "    \"\"\"\n",
        "\n",
        "    # 위치 인덱스 텐서 생성 : 각 배치에서 시퀀스의 길이만큼 위치 인덱스 반복\n",
        "    pos = torch.arrange(src.shape[1]).repeat(src.shape[0], 1).to(self.device)\n",
        "\n",
        "    # 입력 임베딩과 위치 임베딩을 합산해서 입력 텐서 x 생성\n",
        "    x_embedding = self.scale * self.input_embedding(src) + self.pos_embedding(pos)\n",
        "\n",
        "    # 드롭아웃\n",
        "    x_embedding = self.dropout(x_embedding)\n",
        "\n",
        "    # 제일 처음 입력\n",
        "    encoder_output = x_embedding\n",
        "\n",
        "    # 인코더의 에너지 값들을 저장할 비어있는 텐서\n",
        "    atten_encs = torch.tensor([]).to(self.device)\n",
        "\n",
        "    # 각 인코더 레이어를 순차적으로 통과\n",
        "    for layer in self.layers:\n",
        "      # 인코더 레이어를 통과시키면 encoder_output과 에너지 값을 얻는다.\n",
        "      encoder_output, atten_enc = layer(encoder_output, mask)\n",
        "\n",
        "      # atten_map_save가 True면 에너지 저장\n",
        "      if atten_map_save:\n",
        "\n",
        "        # 현재 어텐션 맵을 기존의 atten_encs에 추가 (첫 번째 레이어의 어텐션 맵만 저장 )\n",
        "        atten_encs = torch.cat([atten_encs, atten_enc[0].unsqueeze(0)], dim=0)\n",
        "\n",
        "      # 최종 출력과 에너지 맵을 반환\n",
        "      return encoder_output, atten_encs"
      ],
      "metadata": {
        "id": "7QCld9YPKg6F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Decoder 구현  \n",
        "**Masking 실험**"
      ],
      "metadata": {
        "id": "hglUMTRWQfJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 예제 설정\n",
        "batch_size = 3\n",
        "seq_len = 10\n",
        "padding = 3\n",
        "n_heads = 8\n",
        "\n",
        "# attention_score 텐서 생성 (무작위 값으로 초기화)\n",
        "# Shape : (batch_size, n_heads, seq_len, seq_len)\n",
        "attention_score = torch.randn(batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "# enc_mask 생성 : 패딩이 있는 위치를 마스킹\n",
        "# 각 시퀀스의 마지막 3개 위치에 패딩이 있다고 가정\n",
        "enc_mask = torch.tensor([\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 첫 번째 시퀀스\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 두 번째 시퀀스\n",
        "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1], # 세 번째 시퀀스\n",
        "], dtype = torch.bool).unsqueeze(1).unsqueeze(2) # Shape : (batch_size, 1, 1, seq_len)\n",
        "\n",
        "# enc_mask의 shape을 (batch_size, n_heads, seq_len, seq_len)로 확장\n",
        "enc_mask = enc_mask.expand(batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 전\", \"=\"*25)\n",
        "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\", attention_score[0,0])\n",
        "\n",
        "# attention_score에 enc_mask 적용\n",
        "if enc_mask is not None:\n",
        "  attention_score[enc_mask] = 0 # 마스크된 위치에 매우 작은 값을 넣어 softmax 결과에 영향 미치지 않게 함\n",
        "\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 전\", \"=\"*25)\n",
        "print(\"attention_score shape:\", attention_score.shape)\n",
        "print(enc_mask.shape)\n",
        "\n",
        "# 특정 헤드에 대한 attention_score 확인 (첫 번재 헤드)\n",
        "print(\"=\"*25, \"attention_score에 마스킹 적용 후\", \"=\"*25)\n",
        "print(\"attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\\n\", attention_score[0, 0])"
      ],
      "metadata": {
        "id": "E98shYQqLu0v",
        "outputId": "c1552e2d-87dd-4fc3-92ab-646dd7a5a74b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================= attention_score에 마스킹 적용 전 =========================\n",
            "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
            " tensor([[-0.3395, -0.6395,  0.6110,  0.0265,  0.5371,  1.4998, -0.1269, -1.8718,\n",
            "         -0.8046,  0.9557],\n",
            "        [ 0.6085, -0.0415,  1.0060,  0.3742, -0.7891,  1.5736, -0.8944,  2.0331,\n",
            "         -1.5335, -0.5771],\n",
            "        [-1.1966, -0.8664,  0.1961,  0.4231, -0.3807, -1.5127,  0.2217,  0.5194,\n",
            "         -0.9711, -2.4902],\n",
            "        [-1.0268, -0.3228, -0.0855,  0.7159, -1.0916,  0.4574, -0.8138,  0.8742,\n",
            "         -0.4003, -2.3863],\n",
            "        [-1.5414, -0.0876, -1.8344, -0.3362,  0.3059, -0.5588,  0.1513,  0.9826,\n",
            "          0.0582,  0.6570],\n",
            "        [-1.6296, -1.9270, -0.1551,  0.5691,  1.3528, -0.7154,  0.3098,  0.2853,\n",
            "          0.0462,  2.3199],\n",
            "        [-0.2022,  0.0245,  0.7467, -0.7748,  0.5947, -0.0367, -0.7063,  1.3353,\n",
            "         -0.2909,  3.9718],\n",
            "        [ 0.9530, -0.3280, -0.5247, -0.4833,  0.3841,  0.6340, -1.0267,  0.7190,\n",
            "          0.8408,  0.3102],\n",
            "        [-1.3838,  0.1600,  1.2030, -0.9008, -0.9396,  0.1014, -1.7269,  0.7137,\n",
            "         -1.1408, -0.4086],\n",
            "        [ 0.7245, -0.4016,  2.1362, -0.8443,  1.1298, -2.0315,  2.3670,  0.2509,\n",
            "          0.8094,  0.0294]])\n",
            "========================= attention_score에 마스킹 적용 전 =========================\n",
            "attention_score shape: torch.Size([3, 8, 10, 10])\n",
            "torch.Size([3, 8, 10, 10])\n",
            "========================= attention_score에 마스킹 적용 후 =========================\n",
            "attention_score[0, 0] (첫 번째 배치, 첫 번째 헤드):\n",
            " tensor([[-0.3395, -0.6395,  0.6110,  0.0265,  0.5371,  1.4998, -0.1269,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.6085, -0.0415,  1.0060,  0.3742, -0.7891,  1.5736, -0.8944,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-1.1966, -0.8664,  0.1961,  0.4231, -0.3807, -1.5127,  0.2217,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-1.0268, -0.3228, -0.0855,  0.7159, -1.0916,  0.4574, -0.8138,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-1.5414, -0.0876, -1.8344, -0.3362,  0.3059, -0.5588,  0.1513,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-1.6296, -1.9270, -0.1551,  0.5691,  1.3528, -0.7154,  0.3098,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.2022,  0.0245,  0.7467, -0.7748,  0.5947, -0.0367, -0.7063,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.9530, -0.3280, -0.5247, -0.4833,  0.3841,  0.6340, -1.0267,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-1.3838,  0.1600,  1.2030, -0.9008, -0.9396,  0.1014, -1.7269,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.7245, -0.4016,  2.1362, -0.8443,  1.1298, -2.0315,  2.3670,  0.0000,\n",
            "          0.0000,  0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, n_heads, drop_p):\n",
        "    super().__init__()\n",
        "\n",
        "    # Decoder의 Self-Attention Layer -> Masked Multi Head Attention 정의\n",
        "    self.self_atten = MHA(d_model, n_heads) # forward할 때 Decoder Mask가 따로 부여\n",
        "    self.self_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Encoder - Decoder Attention Layer\n",
        "    self.enc_dec_atten = MHA(d_model, n_heads)\n",
        "    self.enc_dec_atten_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Feed Forward\n",
        "    self.FF = FeedForward(d_model, d_ff, drop_p)\n",
        "    self.FF_LN = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Dropout\n",
        "    self.dropout = nn.Dropout(drop_p)\n",
        "\n",
        "  def forward(self, x, enc_out, dec_mask, enc_dec_mask):\n",
        "    \"\"\"\n",
        "      x : 디코더의 입력 텐서 (batch_size, seq_len, d_model)\n",
        "      enc_out : 인코더의 출력 텐서 (batch_size, seq_len, d_model)\n",
        "      dec_mask : 디코더의 Self-Attention에 사용되는 마스크 (batch_size, 1, seq_len)\n",
        "      enc_dec_mask : 인코더-디코더 Attention에 사용되는 마스크 (batch_size, 1, seq_len)\n",
        "\n",
        "      return : 디코더 레이어의 출력 텐서, 디코더 Self-Attention 맵, 인코더-디코더 Attention 맵\n",
        "    \"\"\"\n",
        "    # 1. 디코더의 Self Attention\n",
        "    residual, atten_dec = self.self_atten(Q=x, K=x, V=x, mask=dec_mask)\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_masked_self_attention_output = self.self_atten_LN(x + residual)\n",
        "\n",
        "    # 2. Encoder - Decoder Attention\n",
        "    # Q는 Masked Multi Head Attention, K, V는 인코더의 출력\n",
        "    residual, atten_dec_enc = self.enc_dec_atten(\n",
        "        Q=decoder_masked_self_attention_output,\n",
        "        K=enc_out,\n",
        "        V=enc_out,\n",
        "        mask=enc_dec_mask\n",
        "    )\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_self_attention_output = self.enc_dec_atten_LN(x + residual)\n",
        "\n",
        "    # 3. Feed Forward\n",
        "    residual = self.FF(decoder_self_attention_output)\n",
        "    residual = self.dropout(residual)\n",
        "    decoder_output = self.FF_LN(decoder_self_attention_output + residual)\n",
        "\n",
        "    # decoder_output : 디코더 출력값. 디코더가 N회 반복된 후 소프트맥스를 만나 단어를 출력하기 위한 값\n",
        "    # atten_dec : Masked Self Attention의 에너지 맵\n",
        "    # atten_dec_enc : Encoder에서 출력한 단어와 Decoder에서 출력할 단어의 에너지 맵\n",
        "    return decoder_output, atten_dec, atten_dec_enc"
      ],
      "metadata": {
        "id": "A07pqWVlSwcu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWigq67RXMYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}